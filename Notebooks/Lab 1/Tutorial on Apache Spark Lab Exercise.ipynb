{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Learning Python and Apache Spark\n",
    "\n",
    "This tutorial will teach you the basics of Python required to use the large-scale data processing framework (Apache Spark).\n",
    "\n",
    "This tutorial assumes basic knowledge of Python\n",
    "\n",
    "##Part 1: Learning the ipynb\n",
    "\n",
    "As you work through a notebook it is important that you run all of the code cells. The notebook is stateful, which means that variables and their values are retained until the notebook is detached (in Databricks Cloud) or the kernel is restarted (in IPython notebooks). If you do not run all of the code cells as you proceed through the notebook, your variables will not be properly initialized and later code might fail. You will also need to rerun any cells that you have modified in order for the changes to be available to other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a Python cell. You can run normal Python code here...\n",
    "print ('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is another Python cell, this time with a variable (x) declaration and an if statement:\n",
    "# Notice the lack of semi-colons and the strict indentation\n",
    "x = 'Hello World!'\n",
    "if len(x) > 5:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do take note that the variable x will exist for the entire notebook once you run the above cell.\n",
    "\n",
    "Now it is your turn! Write a function that takes in a String and returns itself repeated twice.\n",
    "\n",
    "i.e. `repeatTwice('Hello World!')` returns `'Hello World!Hello World!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repeatTwice(x):\n",
    "    return <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to test out your method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert repeatTwice(x)=='Hello World!Hello World!', 'output does not match!'\n",
    "assert repeatTwice('Clarence Ngoh Peng Yu')=='Clarence Ngoh Peng YuClarence Ngoh Peng Yu', 'output does not match!'\n",
    "assert repeatTwice('1243owroewhf8erhjsp8239uroijs')=='1243owroewhf8erhjsp8239uroijs1243owroewhf8erhjsp8239uroijs', 'output does not match!'\n",
    "print ('All passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2: An introduction to using Apache Spark with the Python pySpark API running in the browser\n",
    "\n",
    "In order to use Spark and its API we will need to use a SparkContext. When running Spark, you start a new Spark application by creating a SparkContext. When the SparkContext is created, it asks the master for some cores to use to do work. The master sets these cores aside just for you; they won't be used for other applications. \n",
    "\n",
    "To start using Spark, you would need to initialize the Spark Context, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('appName')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will make a simple word count application and do simple data analytics on it!\n",
    "\n",
    "We'll start with a simple Resilient Distributed Dataset. In Spark, we first create a base RDD. We can then apply one or more transformations to that base RDD. An RDD is immutable, so once it is created, it cannot be changed. As a result, each transformation creates a new RDD. Finally, we can apply one or more actions to the RDDs. Note that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a word RDD from a python list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print (type(wordsRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our RDD a bit, in Spark, we do this with `map` operations, which take in a function.\n",
    "\n",
    "We first have to write the function to transform our dataset. Write a function to append `'.com'` to the end of a string. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def webify(x):\n",
    "    return <FILL IN>\n",
    "\n",
    "print (webify('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert webify('lol') == 'lol.com',  'output does not match!'\n",
    "assert webify('sdufhoadfhasdpfup923') == 'sdufhoadfhasdpfup923.com',  'output does not match!'\n",
    "assert webify('youtube') == 'youtube.com',  'output does not match!'\n",
    "assert webify('x.x.x.x.x') == 'x.x.x.x.x.com',  'output does not match!'\n",
    "print ('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can webify our words RDD and see what it produces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webifiedRDD = wordsRDD.map(<FILL IN>)\n",
    "print (webifiedRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert webifiedRDD.collect() == ['cat.com', 'elephant.com', 'rat.com', 'rat.com', 'cat.com'],  'output does not match!'\n",
    "print ('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should learn a cleaner approach towards writing this code.\n",
    "\n",
    "We introduce lambda functions (anonymous functions), which are of the form \n",
    "\n",
    "`lambda <args> : <method body>`\n",
    "\n",
    "so for instance, if we wanted a map a function that pluralizes all the words in the RDD, we would write the below code\n",
    "\n",
    "`wordsRDD.map(lambda word : word + 's')`\n",
    "\n",
    "Now it's your turn! Write a lambda function that does the exact same thing webify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webifiedRDD = wordsRDD.map(<FILL IN>)\n",
    "print (webifiedRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert webifiedRDD.collect() == ['cat.com', 'elephant.com', 'rat.com', 'rat.com', 'cat.com'],  'output does not match!'\n",
    "print ('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair RDDs \n",
    "\n",
    "The next step in writing our program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, we will create a pair consisting of (`'<word>', 1`) for each word element in the RDD.\n",
    "\n",
    "We can create the pair RDD using the map() transformation with a lambda() function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that map operations may return values of a different type!\n",
    "wordPairs = wordsRDD.map(<FILL IN>)\n",
    "print (wordPairs.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert wordPairs.collect() == [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)],  'output does not match!'\n",
    "print ('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupByKey()` approach **\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`:\n",
    "  + #### The operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "  + #### The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n",
    " \n",
    "Use `groupByKey()` to generate a pair RDD of type `('word', iterator)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that groupByKey requires no parameters\n",
    "wordsGrouped = wordPairs.<FILL IN>\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print ('{0}: {1}'.format(key, list(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST groupByKey() approach \n",
    "assert sorted(wordsGrouped.mapValues(lambda x: list(x)).collect()) == [('cat', [1, 1]), ('elephant', [1]), ('rat', [1, 1])], 'incorrect value for wordsGrouped'\n",
    "print ('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Use `groupByKey()` to obtain the counts **\n",
    "#### Using the `groupByKey()` transformation creates an RDD containing 3 elements, each of which is a pair of a word and a Python iterator.\n",
    "#### Now sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordCountsGrouped = wordsGrouped.<FILL IN>\n",
    "print (wordCountsGrouped.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Use groupByKey() to obtain the counts (2b)\n",
    "assert sorted(wordCountsGrouped.collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)], 'incorrect value for wordCountsGrouped'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Counting using `reduceByKey` **\n",
    "#### A better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(<FILL IN>)\n",
    "print (wordCounts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Counting using reduceByKey (2c)\n",
    "assert sorted(wordCounts.collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)], 'incorrect value for wordCounts'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2d) All together **\n",
    "#### The expert version of the code performs the `map()` to pair RDD, `reduceByKey()` transformation, and `collect` in one statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(<FILL IN>)\n",
    "                       .reduceByKey(<FILL IN>)\n",
    "                       .collect())\n",
    "print (wordCountsCollected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST All together (2d)\n",
    "assert sorted(wordCountsCollected) == [('cat', 2), ('elephant', 1), ('rat', 2)], 'incorrect value for wordCountsCollected'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Finding unique words and a mean value **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Unique words **\n",
    "#### Calculate the number of words that are unique (i.e. only exists once) in `wordsRDD`.  You can use other RDDs that you have already created to make this easier. \n",
    "Hint: use the `filter` operation with the `wordCountsGrouped` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "uniqueWords = wordCountsGrouped.<FILL IN>\n",
    "print (uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Unique words (3a)\n",
    "assert uniqueWords == 1, 'incorrect count of uniqueWords'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Mean using `reduce` **\n",
    "#### Find the mean number of words per word in `wordCounts`.\n",
    "#### Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words.  First `map()` the pair RDD `wordCounts`, which consists of (key, value) pairs, to an RDD of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from operator import add\n",
    "totalCount = (wordCounts\n",
    "              .map(<FILL IN>)\n",
    "              .reduce(<FILL IN>)\n",
    "numWords = wordCounts.<FILL IN>\n",
    "average = totalCount / float(numWords)\n",
    "print (totalCount)\n",
    "print (round(average, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Mean using reduce (3b)\n",
    "assert round(average, 2) == 1.67, 'incorrect value of average'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: Analysing real datasets **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section we will make use of real data from a csv file, load in our data source, and do some analytics with Apache Spark on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4a) Load a text file **\n",
    "#### For the next part of this lab, we will use the [Real Estate Transactions](http://samplecsvs.s3.amazonaws.com/Sacramentorealestatetransactions.csv) from [SpatialKey Sample CSV Data](https://support.spatialkey.com/spatialkey-sample-csv-data/). The Sacramento real estate transactions file is a list of 985 real estate transactions in the Sacramento area reported over a five-day period, as reported by the Sacramento Bee. Note that this file has address level information that you can choose to geocode, or you can use the existing latitude/longitude in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just run this code\n",
    "import os.path\n",
    "from pyspark.sql import Row\n",
    "#import pyspark_csv as pycsv\n",
    "\n",
    "baseDir = os.path.join('/tmp')\n",
    "inputPath = os.path.join('data', 'Sacramentorealestatetransactions.csv')\n",
    "csvFileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "#attributes are from the first row\n",
    "\n",
    "sacramentoWithHeaderRDD = (sc.textFile('/tmp/data/Sacramentorealestatetransactions.csv')\n",
    "                .map(lambda line : line.split(','))\n",
    "                .map(lambda x : Row(\n",
    "            street = x[0],\n",
    "            city = x[1],\n",
    "            zipcode = x[2],\n",
    "            state = x[3],\n",
    "            beds = x[4],\n",
    "            baths = x[5],\n",
    "            sqfeet = x[6],\n",
    "            typeOfHouse = x[7],\n",
    "            saleDate = x[8],\n",
    "            price = x[9],\n",
    "            latitude = x[10],\n",
    "            longitude = x[11]\n",
    "        )))\n",
    "header = sacramentoWithHeaderRDD.take(1)[0]\n",
    "sacramentoRDD = sacramentoWithHeaderRDD.filter(lambda line: line != header)\n",
    "print (sacramentoRDD.top(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4b) Find median, mean and standard deviation of the prices **\n",
    "####As a real estate company, you may be interested in the mean and standard deviation of the prices of the houses in the area. In this section, we will be referencing the Apache PySpark API to calculate these values.\n",
    "\n",
    "[Apache PySpark API](http://spark.apache.org/docs/latest/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map the RDD to prices\n",
    "sacramentoPricesRDD = sacramentoRDD.map(<FILL IN>)\n",
    "\n",
    "#Find the mean\n",
    "mean = sacramentoPricesRDD.mean()\n",
    "print (mean)\n",
    "\n",
    "#Find the standard deviation\n",
    "stDev = sacramentoPricesRDD.stdev()\n",
    "print (stDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Mean, StDev (4b)\n",
    "assert mean == 234144.26395939104, \"invalid mean\"\n",
    "assert round(stDev,2) == 138295.58, \"invalid stDev\"\n",
    "print(\"All passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4c) Find the most expensive house **\n",
    "####Now let's try to find the most expensive house in the region. You will be using the `takeOrdered()` for this. takeOrdered takes in an integer n of the number of houses you want to search. The key is the function to be applied to the elements before they are sorted in ascending order.\n",
    "\n",
    "[Apache PySpark API](http://spark.apache.org/docs/latest/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostExpensiveHouse = sacramentoRDD.takeOrdered(1, lambda x: -float(x.price))\n",
    "print (mostExpensiveHouse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert mostExpensiveHouse[0].price == str(884790), \"invalid most expensive house\"\n",
    "print ('All tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4d) Find the top-k cheapest house **\n",
    "####Now let's try to find the top-k cheapest houses in the region. The top-k notation is just a way of saying that you are taking the first `k` number of entries. You will be using the `takeOrdered()` for this. takeOrdered takes in an integer n of the number of houses you want to search. The key is the function to be applied to the elements before they are sorted in ascending order.\n",
    "\n",
    "[Apache PySpark API](http://spark.apache.org/docs/latest/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#topKCheapestHouses takes in an integer k and returns a list of k entries of houses\n",
    "def topKCheapestHouses (k):\n",
    "    kCheapestHouses = sacramentoRDD.takeOrdered(k, lambda x: float(x.price))\n",
    "    return kCheapestHouses\n",
    "\n",
    "print (topKCheapestHouses(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert topKCheapestHouses(10) == [Row(baths='3', beds='3', city='LINCOLN', latitude='38.851645', longitude='-121.231742', price='1551', saleDate='Fri May 16 00:00:00 EDT 2008', sqfeet='0', state='CA', street='3720 VISTA DE MADERA', typeOfHouse='Residential', zipcode='95648'), Row(baths='4', beds='3', city='SLOUGHHOUSE', latitude='38.490447', longitude='-121.129337', price='2000', saleDate='Fri May 16 00:00:00 EDT 2008', sqfeet='5822', state='CA', street='14151 INDIO DR', typeOfHouse='Residential', zipcode='95683'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.885327', longitude='-121.289412', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='20 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.885132', longitude='-121.289405', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='24 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.884936', longitude='-121.289397', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='28 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.884741', longitude='-121.28939', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='32 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.884599', longitude='-121.289406', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='36 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.884535', longitude='-121.289619', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='40 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.88459', longitude='-121.289835', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='44 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648'), Row(baths='0', beds='0', city='LINCOLN', latitude='38.884667', longitude='-121.289896', price='4897', saleDate='Mon May 19 00:00:00 EDT 2008', sqfeet='0', state='CA', street='48 CRYSTALWOOD CIR', typeOfHouse='Residential', zipcode='95648')], \"incorrect top-k cheapest function\"\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4e) Data visualization with matplotlib **\n",
    "####Now let's try to visualize the price data in a box-plot. This functionality is provided by matplotlib. We have provided the code here for you, but feel free to explore the API and see what other data you can gather from this dataset!\n",
    "\n",
    "[Matplotlib API](http://matplotlib.org/contents.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just run the code!\n",
    "#required to print inline\n",
    "%matplotlib inline  \n",
    "\n",
    "#import the libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#map the data to prices and plot\n",
    "data = sacramentoRDD.map(lambda house : float(house.price)).collect()\n",
    "plt.boxplot(data, notch=0, sym='+', vert=1, whis=1.5)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
